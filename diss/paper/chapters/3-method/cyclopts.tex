
\section{Experimental Tools}\label{method:tools}

In order to explore the large number of possible exchange instances described in
\S \ref{method:setup}, a sophisticated problem solving framework is needed. This
section describes the design principles and implementation details of a new
software package called Cyclopts (\underline{Cycl}us \underline{Opt}imization
\underline{S}tudies). Cyclopts, written primarily in Python with a C++ layer
used to operate with Cyclus, provides a general framework for sampling a
parameter space, defining problem instances for a given point in parameter
space, and solving a problem instance under a variety of conditions.

The section begins with a short discussion on terminology in \S
\ref{method:tools:term}.  \S \ref{method:tools:struc} describes the general
Cyclopts workflow and specific features used in the DRE experimental
campaign. \S \ref{method:tools:hdf5} discusses Cyclopts persistence mechanisms,
including database design and layout. The section concludes with \S \ref{},
describing Cyclopts' high throughput computing (HTC) capability, which enables
scalable, concurrent execution.

\subsection{Terminology}\label{method:tools:term}

Cyclopts supports a two-tier definition of problem instances, borrowing terms
from biological classification. Problem \textit{families} describe a general
form of problem instance. For example, the Traveling Salesman Problem (TSP)
could be implemented as a problem family. In this analysis, the NFCTP is
considered the problem family, since any given instance of the NFCTP will have
the same general structure. Whether or not the LP or MILP formulation is used is
dependent on whether or not arcs in the Exchange Graph are labeled as exclusive
or not. If there are no exclusive arcs, the LP formulation is used; otherwise,
the MILP formulation is used.

Each problem family can have any number of \textit{species}. One can
conceptualize the relationship as a tree structure, in which families are parent
nodes and species are child nodes. A problem species defines the methodology for
generating \textit{instances} of a problem family. Using the TSP example above,
a problem species may be ``the greater Atlanta metropolitan area'', for which
the effect of regional gas prices may be studied. For the NFCTP study, front-end
and back-end exchanges form two separate species. Each species can have unique
parameters in addition to family-related parameters, is the case for the two
species studied.

\subsection{Design}\label{method:tools:struc}

The full Cyclopts stack is comprised of three phases: generation of parameter
space, generation of instances, and execution of instances. The workflow begins
with user input detailing a range of values for a set of parameters. Cyclopts
then translates the input into a parameter space by enumerating all possible
combinations of parameters. For example, if parameters $x$ and $y$ have defined
values of $[1, 2]$ and $[3, 4, 5]$, respectively, Cyclopts will generate a
parameter space comprised of six points in $(x, y)$ notation: $(1, 3)$, $(1,
4)$, $(1, 5)$, $(2, 3)$, $(2, 4)$, and $(2, 5)$. Each point is then then
provided to a problem species in order to generate one or more problem
instances. Species are expected to define defaults for all parameters as user
input may define values for only a subset of available parameters.

Given a point in parameter space, an instance can be generated. If there are any
stochastic effects during instance generation, many instances may be
generated. Again, because parameters are species dependent, the logic of
instance generation from a set of parameters is the task of a problem
species. Following instance generation, instances may be executed. Cyclopts
supports multiple solution options by design. The same instance may be solved
with both a heuristic and a full optimization solver, for example. Once an
instance of a problem is defined, it is independent of any species-level
effects. Accordingly, instance execution and related logic is the domain of
problem families. 

A summary of the high-level Cyclopts workflow and entities is presented in
Figure \ref{fig:lopts_desgin}. Note that objects generated as the workflow moves
from parameter space to instance solution form a tree structure.

\TODO{update fig}
\begin{figure}
  \begin{center}
    \includegraphics[width=0.55\textwidth]{exchange_part_supreq.pdf}
    \caption[]{
      \label{fig:lopts_desgin}
      Foo.}
  \end{center}
\end{figure}

\subsection{Persistence Mechanisms}\label{method:tools:hdf5}

While the root node in Figure \ref{fig:lopts_desgin} is generated from a
user-provided input file, each subsequent level in the hierarchy represents a
stateful object: a point in parameter space, a problem instance, and a
solution. Each stateful object can be written to and read from disc. Cyclopts
also incorporates a post-processing step, during which all related objects may
be analyzed and aggregate data may be collected and written to disc. While any
input/output (I/O) persistence mechanism is valid, Cyclopts is currently
implemented using Hierarchical Datat Format 5 (HDF5) \cite{hdf5} via PyTables
\cite{pytables}.

Data in HDF5 is stored hierarchicaly, similar to a filesystem. At the root node
of the filesystem-like structure, a \textit{group} is defined for problem family
and problem species data, named \code{Family} and \code{Species},
respectively. A \textit{dataset} for aggregate results named \code{Results} is
also defined. A path in HDF5 is designated in a UNIX-like manner. For example,
the path to \code{Family} would be \code{/Family}, indicating that the group is
directly under the root node, \code{/}. Further, groups are defined for each
kind of family and species. The DRE problem family records data in the group
\code{/Family/ResourceExchange}, front-end exchanges record data in the group
\code{/Species/StructuredRequest}, and back-end exchanges record data in the
group \code{/Species/StructuredSupply}. 

Each stateful object is given a Universally Unique Identifier (UUID) by which it
can be identified for future reading and analysis. The UUID is used in two
distinct capacities: as a \textit{primary key} in a dataset for future
identification or as the name of a group. Whether to aggregate data in one large
dataset or divide data into datasets for each object is a design decision
informed by practical performance. A study of the tradeoffs between each
approach is presented in \S \ref{method:tools:hdf5:study}. As a result of that
study, for objects that are both read and written, the latter approach is taken.

\subsubsection{Parameter Space}

Both front-end and back-end species record the state of every point in a given
parameter space in a data set called \code{/Species/<species type>/Points},
where \code{<species type>} is either \code{StructuredRequest} or
\code{StructuredSupply}. Each point incorporates both fundamental and instance
parameters as described in \S \ref{method:setup}.

\TODO{add table for each?}

\subsubsection{Problem Instances}

Problem instances are generated by problem species and are executed by problem
families. Accordingly, both species and families can record information about
instances. Front and back-end exchange species each record two types of
information: details about each arc in an instance and a summary of
species-specific information. The exchange family records information regarding
each of the entities that comprise an instance: nodes, groups of nodes (having
been translated from portfolios), and arcs. Further, aggregate summary
information is also recorded. 

\paragraph{Exchange Species}

Both exchange species record information about each arc in an exchange instance
detailed in Table \ref{tbl:sp_inst_arc}. A parent group for arc data is defined
under each species group. A group for each instance, whose name is the hex
string of the UUID, is defined under the associated arc group. Finally, arc
information associated with each instance is stored as a dataset in that
instance's group. For example, the arc data for a given UUID of a front-end
exchange is located as a dataset in the group
\code{/Species/StructuredRequest/Arcs/<UUID hex>}.

\begin{table}[]
\centering
\label{tbl:sp_inst_arc}
\caption{Data recorded for every arc in an instance.}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Name} & \textbf{Data Type} & \textbf{Description}       \\ \hline
arcid         & 32-byte integer    & ID for an arc              \\ \hline
commod        & 32-byte integer    & ID for a commodity         \\ \hline
pref\_c       & 32-byte float      & Commodity-based preference \\ \hline
pref\_l       & 32-byte float      & Location-based preference  \\ \hline
\end{tabular}
\end{table}

Summary information related to each species is also recorded in a data set for
each species type located in the group \code{/Species/<species
  type>/Summary}. Table \ref{} describes the data recorded for front-end
exchanges, and Table \ref{} describes the data recorded for back-end exchanges.

\TODO{write both tables}

\paragraph{Exchange Family}

The exchange family records information regarding all major constructs in an
exchange: nodes, groups, and arcs. Nodes and group data are recorded in an
aggregate dataset located at \code{/Family/ResourceExchange/ExchangeNodes}. A
summary of the node datastructure is given in Tables \ref{}; the node group
data, located at \code{/Family/ResourceExchange/ExchangeGroups}, is summarized
in Table \ref{}. Arc data is collected in the group
\code{/Family/ResourceExchange/ExchangeArcs}. A dataset per instance UUID is
used because it has been found to be useful in the postprocessing phase. A
summary of the arc dataset structure is provided in Table \ref{}.

\TODO{Table for each}

\subsubsection{Solutions}

For every solution, data is added to the Cyclopts Results dataset. The structure
of the Results dataset is described in Table \ref{}. Problem solutions are
constructed from problem instances, and are thus managed by a problem
family. Aggregate solution information is provided in a family dataset
\code{/Family/ResourceExchange/ExchangeSolutionProperties}, the structure of
which is detailed in Table \ref{}. The full results of each solve, i.e., the
amount of resources flowing across each arc, are recorded in a group specific to
each solution UUID. The structure of the detailed results are provided in Table
\ref{}.

\TODO{Table for each}

\subsubsection{Post Processing}

Given a full set of parameter, instance, and solution data, postprocessing may
be applied to family and species data. The exchange family, front-end species,
and back-end species each contain a \code{PostProcess} dataset. The exchange
family dataset structure is described in Table \ref{}. Both exchange species
share the same dataset structure, outline in Table \ref{}.

\TODO{Table for each}

\subsubsection{Summary Database Layout}

The database hierarchical structure for exchange data, assuming both exchange
species are included, is shown in Figure \ref{}.

\subsubsection{Performance Studies}\label{method:tools:hdf5:study}

\textit{Chunk size} is a critical parameter of HDF5 datasets that affects I/O
performance. HDF5's storage layout is not contiguous, rather, data is separated
into equal-sized \textit{chunks}. Any reading or writing occurs on a chunk of
data, rather than accesing an entire dataset. Accordingly, choosing a reasonable
chunk size can greatly increase performance for known data access operations. In
PyTables, the \textit{compresison level} of a dataset is also a tuneable
parameter that affects I/O performance. Compression, of course, reduces overall
database size. Therefore, an ideal compression is the largest possible that
retains acceptable performance.

Originally, all Cyclopts datasets used a UUID-as-primary-key layout. For
instance, rather than many tables with a layout described in Table
\ref{tbl:sp_inst_arc}, all related tables were combined and included an extra
column naming the instance UUID. However, extremely long read times were
encountered when post processing data. The basic procedure for performing a
post-process operation included reading all rows associated with a UUID in an
input exchange family dataset, reading all rows associated with the same UUID in
an output exchange species dataset, selecting a value from each row, and
performing a dot product of the resulting vectors. 

In order to investigate possible chunk size and compression optimizations, a
small ($\sim$ MBs) dataset and a large ($\sim$ GBs) dataset were created. The
postprocessing step was then run on 25 instances in each dataset. The operation
was timed using the UNIX \code{time} command. Poor performance was experience
with an initial chunksize proportional to the ratio of a normal L2 cache to row
size was used for each dataset and a compression level of four was selected per
suggestions from the PyTables documentation \cite{tablesopt}. Accordingly, chunk
size and compression level were varied around these recommended values in order
to determine if any tuning was available. The results of the study on the small
dataset is shown in Figure \ref{fig:small_db}. The large dataset results is
shown in Figure \ref{fig:large_db}.

\begin{figure}
  \begin{center}
    \includegraphics[width=0.85\textwidth]{small_all.pdf}
    \caption[]{
      \label{fig:small_db}
      Postprocessing performance for 25 entries of a small-sized database for a 
      variety of compression levels and chunk sizes.}
  \end{center}
\end{figure}

\begin{figure}
  \begin{center}
    \includegraphics[width=0.85\textwidth]{large_all.pdf}
    \caption[]{
      \label{fig:large_db}
      Postprocessing performance for 25 entries of a large-sized database for a
      variety of compression levels and chunk sizes.}
  \end{center}
\end{figure}

Assuming some level of compression, an ideal chunksize range is identified for
the small database of between $\sim 10^3 - 10^5$ bytes. Further the small
database example confirms that the study's methodology is well founded: an ideal
chunksize range is established. A similar optimal chunk size range is found for
the large database. However, note that in this exercise, only $\sim 0.25\%$ of
instances are postprocessed. An optimal performance of $> 80$ seconds per
instance is unacceptable.

A number of strategies exist for trying to increase performance. A classic
strategy is pivoting the group-dataset structure such that data queries are made
upon a group rather than rows in a dataset. In this example, such a pivot
involves dividing the single, large dataset into $n$ datasets, where $n$ is the
number of unique primary keys (UUIDs in this case). 

Accordingly, an additional test was conducted such that all datasets on which
queries are made were pivoted such that new group nodes were added for each
UUID, and all data for that UUID was appended to a dataset under the associated
group. The post processing step was divided into the read operations associated
with the exchange family and the the read operations associated with a
species. The exact same operations were then tested on a large database with the
column-based layout and a large database with the group-based layout. Specific
instances, increasing in size, were identified to be postprocessed. The
group-based results were compared with the column-based results and are shown in
Figure \ref{}.

\begin{figure}
  \begin{center}
    \includegraphics[width=0.85\textwidth]{grp_v_col_ratio.pdf}
    \caption[]{
      \label{fig:col_grp}
      The ratio of group-base d queries to column-based queries as a function of
      problem size. A lower ratio indicates a faster process time for the group
      strategy over the column strategy.}
  \end{center}
\end{figure}

As can be seen, the group-based strategy performs quite well, almost an order of
magnitude better than the column-based strategy. Furthermore, species operations
are shown to have a much larger speedup relative to family operations. This is
due to the fact that at the time of this analysis, solution values were stored
\textit{only if} they were nonzero. When read, a datastructure must be allocated
and populated for each non-zero index rather than simply copying a block on data
on disc. The writing of family-based solution values has since been updated to
also write zero values to avoid this issue.

For the purposes of this study, the dataset-group pivot served the required
purpose. Postprocessing now performs satisfactorily for the operations needed
and the database sizes experienced. However, if future performance issues arise,
other strategies may be investigated. Perhaps the most fruitful of these will be
returning to the single datatset layout and using PyTable's indexing feature. 

\subsection{Implementation}

Cyclopts defines abstract application programming interfaces (APIs) for both
families and species in the \code{ProblemFamily} and \code{ProblemSpecies}
classes, respectively. While many parts of an API are related to the workflow
discussed in \S \ref{method:tools:struc:des}, others are related to the
persistence mechanisms discussed in \S \ref{method:tools:hdf5}. The full API is
described in detail in the Cyclopts documentation \cite{cyclopts}.

Problem families are responsible for reading instances from a database, executing instances given a solver configuration, and writing solution 

%% 00023 static inline double CoinCpuTime()
%% 00024 {
%% 00025   double cpu_temp;
%% 00026 #if defined(_MSC_VER)
%% 00027   unsigned int ticksnow;        /* clock_t is same as int */
%% 00028   
%% 00029   ticksnow = (unsigned int)clock();
%% 00030   
%% 00031   cpu_temp = (double)((double)ticksnow/CLOCKS_PER_SEC);
%% 00032 #else
%% 00033   struct rusage usage;
%% 00034   getrusage(RUSAGE_SELF,&usage);
%% 00035   cpu_temp = usage.ru_utime.tv_sec;
%% 00036   cpu_temp += 1.0e-6*((double) usage.ru_utime.tv_usec);
%% 00037 #endif
%% 00038   return cpu_temp;
%% 00039 }

\subsection{High Throughput Computing}\label{method:tools:htc}

Things \cite{bui_work_2011} \cite{cde} \cite{condor-practice}.

\subsection{Command Line Interface}
