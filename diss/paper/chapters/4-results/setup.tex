
\section{Experimental Setup}\label{results:setup}

An experiment consists of a set of resource-exchange graph instances executed
with a collection of configured solvers. When a solution is found, the solution
(i.e., the flow vector), the time required to reach the solution, the objective
value (i.e., the dot-product of cost and flow vectors), and the simulation
objective value (i.e., the dot-product of preference and flow vectors), are
recorded. Because solution time is a quantity of interest, all instances in an
experiment must be executed on homogeneous architecture. Furthermore, all
experiments must be executed on equivalent systems in order to quantify valid
comparisons in solutions times across experimental campaigns.

Six execution nodes on UW-Madison Center for High-Throughput Computing (CHTC)
HTCondor system form the homogeneous environment used to conduct the experiments
herein described. Each execute node is comprised of a 2.90 GHz eight-core Intel
Xeon E5-2690 \cite{intelproc} processor with 128 GB of RAM. Processor
hyper-threading was disabled for the duration of the experimental campaign to
allow comparisons between solution times.

For each experimental study, an input database consisting of persisted resource
exchange graph instances is generated. A copy of the database is transferred
from a user's submit node to each of the six execution nodes. A WorkQueue master
process is initiated. For every execute node, workers are initialized using
WorkQueue's \texttt{condor\_submit\_workers} CLI. The master maintains a queue
of instances to be solved, assigning instances to workers as workers become
available. Upon completion, the input database is removed from each execution
node, and the results are collected from the user's submit node. The resulting
database is then post-processed and analyzed.

\subsection{Solvers and Formulations}

Three solvers are executed for each resource exchange graph instance: the Greedy
Heuristic, described in \secref{abm:dre:nfctp:heur}, COIN's LP solver
(\clp), and COIN's branch-and-cut solver (\cbc). Each problem instance
is constructed as an \texttt{ExchangeGraph}, i.e., at the \textit{exchange
  layer} shown in Figure \ref{fig:dre_impl} and Figure \ref{fig:dre_time}. The
Greedy Heuristic is applied directly to the \texttt{ExchangeGraph}. The CLP and
\cbc solvers require a translation to the \textit{formulation layer}. The CLP
solver is applied to the LP formulation of the NFCTP and the \cbc solver is
applied to the MILP formulation. The solution time, $t_s$ of a given solver is
defined as the time required to return a vector of arc flows given an
\texttt{ExchangeGraph} instance as shown in Figure \ref{fig:dre_time}.

\begin{equation}\label{eqn:solnt}
t_s = t_f - t_i
\end{equation}

\begin{figure}
  \begin{center}
    \includegraphics[width=\textwidth]{exchange_xlation_timing.pdf}
    \caption{
      \label{fig:dre_time}
      The time points for comparing different solutions using Equation \ref{eqn:solnt}.}
  \end{center}
\end{figure}

The Greedy Heuristic has linear-like scaling. Each request portfolio is treated
in the algorithm. For each request node in the portfolio, incoming arcs are
sorted, which is a $\mathcal{O}(n\log{}n)$ operation. The algorithm terminates
when each request portfolio has either been satisfied or found to be
unsatisfiable. In the worst case, every node is perused, resulting in
$\mathcal{O}(n\log{}\frac{n}{N})$, with $n$ arcs and $N$ nodes. This scaling is
better than $\mathcal{O}(n\log{}n)$ because arcs in the system are
partitionable. As both CLP and \cbc solve NP-Hard problems, there is no
\textit{a priori} expected performance behavior. However, LP problems solved
with the Simplex method are known in practice to scale by the number of
constraints in the system. As described in Appendix \ref{app:lp}, the method
iterates over vertices of the polytope defining a instance's feasible space,
which are defined by constraints. MILP problems must be solved by enumeration,
as shown in Appendix \ref{app:ip}, thus their scaling is chiefly a function of
the number of instance variables.

\subsection{System Parameters}

\Secref{method:setup:params} describes the parameters defining both Front
and Back-End exchanges. Each combination of fundamental parameters represents a
significant modeling assumption. Therefore, every experiment is conducted for
every combination of fundamental parameters, comprising eighteen combinations in
total. For each exchange type, a reference instance parameter vector is
chosen. 

Reference instance parameter vectors for front and back-end exchanges are shown
in Tables \ref{tbl:front_ref_params} and \ref{tbl:back_ref_params},
respectively. Reactor population and core composition values were chosen in line
with what a user might reasonably find in a simulation, such as a 75\%-25\%
thermal-to-fast reactor split and 33\% possible MOX-fuel residency in thermal
reactors. Support facility population values were chosen to model a ``worst
case'' simulation. A ``best case'' simulation is one in which reactor supply and
demand is evenly distributed. For instance, a supporting facility with monthly
capacity values that can support two reactors a month is capable of supporting
twenty-four reactors a year. A ``worst case'' simulation is one in which there
are time steps where all reactors interact with the DRE simultaneously. Such a
simulation will have a number of supporters commiserate with the total number of
reactors. These parameter vectors are associated with such ``worst case''
timesteps.

\begin{table}[h!]
\centering
\caption{Reference Values for Front-End Exchange Instance Parameters.}
\label{tbl:front_ref_params}
\begin{tabular}{|c|c|}
\hline
Parameter    & Reference Value
\\ \hline
$r_{rx, \text{Th}}$   & 0.75 
\\ \hline
$r_{rx, \text{FThOX}}$ & 0.25
\\ \hline
$r_{l, c}$ & 1
\\ \hline
$f_{mox}$     & 0.33
\\ \hline
$r_{s, \text{Th}}$ & 0.08
\\ \hline
$r_{s, \text{TMOX}, \text{UOX}}$ & 1.
\\ \hline
$r_{s, \text{FMOX}}$ & 0.2
\\ \hline
$r_{s, \text{FThOX}}$ & 0.2
\\ \hline
$r_{inv, proc}$   & 1
\\ \hline
\end{tabular}
\end{table}

\begin{table}[h!]
\centering
\caption{Reference Values for Back-End Exchange Instance Parameters.}
\label{tbl:back_ref_params}
\begin{tabular}{|c|c|}
\hline
Parameter    & Reference Value
\\ \hline
$r_{rx, \text{Th}}$   & 0.75 
\\ \hline
$r_{rx, \text{FThOX}}$ & 0.25
\\ \hline
$r_{l, c}$ & 1
\\ \hline
$f_{mox}$     & 0.33
\\ \hline
$r_{s, \text{Th}}$ & 0.08
\\ \hline
$r_{s, \text{TMOX}, \text{UOX}}$ & 1.
\\ \hline
$r_{s, \text{FMOX}}$ & 0.2
\\ \hline
$r_{s, \text{FThOX}}$ & 0.2
\\ \hline
$r_{s, \text{Repo}}$   & 0.2
\\ \hline
$d_{\text{Th}}$   & {UOX: 2/3, TMOX: 1/3, FMOX: 0}
\\ \hline
$d_{\text{FMOX}}$   & {UOX: 1/4, TMOX: 0, FMOX: 3/4, FThOX: 0}
\\ \hline
$d_{\text{FThOX}}$   & {UOX: 1/4, TMOX: 0, FMOX: 0, FThOX: 3/4}
\\ \hline
\end{tabular}
\end{table}

\subsection{Analysis Metrics}

The most obvious metrics to compare between solutions is the solution time,
$t_s$, and objective function value $z$,

\begin{equation}\label{eqn:obj_flow}
z = \sum_{(i, j) \in A} c_{i, j} x_{i, j}.
\end{equation}

For any given instance, the optimal objective value, $z^*$, is associated with a
set of flows, $X^*$. By definition, an optimal solution will have a lower system
cost than or an equivalent system cost to any feasible solution. By necessity,
any solution to the NFCTP will include flows along false arcs if such flows
exist. One can also consider solution metrics that only account for arcs that
exist in a simulation, $A_{\text{sim}}$. Because the simulation opperates in
preference-space, a ``simulation objective'' is defined as

\begin{equation}\label{eqn:sim_flow}
z_{\text{sim}} = \sum_{(i, j) \in A_{\text{sim}}} p_{i, j} x_{i, j},
\end{equation}

\noindent 
and the simulation objective associated with an optimal solution is defined as

\begin{equation}\label{eqn:sim_flow}
z^*_{\text{sim}} = \sum_{(i, j) \in A_{\text{sim}}} p_{i, j} x^*_{i, j},
\end{equation}

While $z^* \leq z$ is true in cost space, $z^*_{\text{sim}} \geq z_{\text{sim}}$
is \textit{not} necessarily true in practice for MILP instances. Any MILP solver
must use some convergence criteria, which takes into account flow values along
false arcs. 

%% The preference vector, $\vec{p}$, can be divided into its two components,
%% $\vec{p} = \vec{p}_c + \vec{p}_l$. If $f_{\text{loc}}$ is zero, then, by
%% definition, $\vec{p}_l = 0$, otherwise, it is comprised of non-zero
%% components. $\vec{p}_c$, however, is invariant under all values of
%% $f_{\text{loc}}$. Therefore, the commodity contribution of the simulation
%% objective, shown in Equation \ref{eqn:cpref_flow} is also an interesting
%% metric. It allows for direct investigation of the effect of adding
%% location-based preferences to the system. Furthermore, comparisons can be made
%% between systems with coarse and fine location-based preferences using this
%% metric.

%% \begin{equation}\label{eqn:cpref_flow}
%% z^c_{\text{sim}} = \sum_{(i, j) \in A_{\text{sim}}} p^c_{i, j} x_{i, j}.
%% \end{equation}

Each of the above metrics compare aggregate values between solutions. For any
two solutions to identical problem instances, though, more detailed comparisons
could be made. The individual flow values, and values derived therefrom, can be
compared directly using well-known normative measure. One example is the root
mean square (RMS) of constituents of the objective function, shown in Equation
\ref{eqn:rms_flow}.

\begin{equation}\label{eqn:rms_flow}
RMS_{\text{z}} = \sqrt{ \frac{1}{N} \sum_i c_i^2 (x_{i, 1} - x_{i, 2}) ^2 }
\end{equation}

While such an analysis is common in the realm of nuclear engineering, it is less
appropriate for comparing solutions to optimization problems. An instance of an
optimization problem may be \textit{degenerate}, i.e., it may have multiple
equivalent optimal solutions. Consider a two-arc system with arc costs of unity
and a total flow constraint of unity. A spectrum equivalent solutions exist
between $(0, 1)$ and $(1, 0)$. When comparing any solution, the difference in
objective function value is, necessarily, zero. However, using an RMS analysis,
the solutions of $(0, 1)$ and $(1, 0)$ would report the largest possible
error. Therefore, only metrics that involve aggregate measures of solutions are
used in the following analysis.
