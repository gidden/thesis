There have been a number of different benchmarking exercises attempted by
governmental and international agencies to date. The most notable of these are
the International Atomic Energy Agency (IAEA) International Project on Nuclear
Reactors and Fuel Cycle's (INPRO) benchmark exercise \cite{_international_2009},
the Massachusetts Institute of -Technology's benchmark exercise
\cite{guerin_benchmark_2009}, the Nuclear Waste Technology Review Board's
(NWTRB) benchmark exercise \cite{abkowitz_workshop_2011,_nuclear_2011} and the
Organization for Economic Cooperation and Development's (OECD) benchmark
exercise \cite{boucher_benchmark_2012}.

Each of the benchmarking efforts used a different set of codes by which to
compare against one another. The codes used in each is described in
Table \ref{tab:benchmark-codes}.

\begin{table} [h!]
\centering
\begin{tabular} {|c|c|} 
\hline
Benchmark    & Codes Used \\
\hline
IAEA - INPRO & COSI, DESAE, FAMILY, TAPS, VISTA, VISION \\
MIT          & CAFCA, COSI, DANESS, VISION \\
NWTRB        & \textit{AREVA}, CAFCA, NUWASTE, VISION \\
OECD         & COSI, EVOLCODE, DESAE, FAMILY, VISION \\
\hline
\end{tabular}
\caption{The Set of Codes Compared in Each Benchmarking Effort}
\label{tab:benchmark-codes}
\end{table}

Each benchmarking effort investigated a different set of scenarios. The chosen
scenarios depend heavily on the interest of the organizing institution. For
example, the NWTRB is a congressionally mandated review board, thus their
scenario suite focused on options for the U.S. fleet of nuclear reactors. The
IAEA is interested in general nation-based trends, so INPRO benchmarks called
for scenarios with low, moderate, and high nuclear power capacity growth. How
the scenarios for these benchmarks are actually described, i.e., how their input
parameters are provided to the various simulation teams, vary for each
exercise. At one end of the spectrum is the INPRO benchmark. Investigation of
the benchmark specification document \cite{_international_2009} shows minimal
information regarding reactor growth rates. All details regarding reactor
parameters, supporting facility parameters, and fuel isotopic information were
discussed ``off line'' and is housed in a database not available to the
public. The NWTRB benchmarking effort suffers a similar lack of
information. They provide some publicly accessible information regarding their
benchmark specification \cite{abkowitz_scenario_2011}, but do not provide enough
information to independently run the benchmark. This is further evidenced by the
participants, who acknowledge that the process starts with ``general specs''
which often ``reflect the peculiarities of one of the models involved'' and are
then processed over many iterative cycles \cite{piet_vision_2011}, leading to an
often incomplete specification whose final published record may not reflect the
final agreed-upon parameters. The MIT benchmark provides more information than
the previous two, including parameters related to reactor and supporting
facilities as well as limited information regarding fuel characteristics, such
as burnup. It is, however, not complete with respect to isotopic information
(which they state is due to ``data availability''
\cite{guerin_benchmark_2009}). Further, it suffers from the problem stated
above: it is a specification designed for CAFCA, the host simulator. It should
be noted that such statement is not a criticism, simply an observation; this is
how the current fuel cycle simulation benchmark works. Current work is being
performed to attempt to tackle such obstacle (see
\S\ref{sec:prev-benchmark}). Finally, the OECD benchmark is the most well
defined. In fact, an entire document was dedicated to its specification a full
four years prior to the release of the benchmarking results
\cite{boucher_specification_2008}. The benchmark includes isotopic details of
all input and output fuels, information about both reactor and support
facilities, and information regarding the facility deployment. It is also one of
the simpler suite of benchmarks among the group.

The fuel cycle simulation community does not benefit from benchmarking exercises
in the same way that the nuclear physics community benefits from them. For
example, the Monte Carlo N-Particle (MCNP) code community has a number of
criticality safety benchmarks \cite{wagner_mcnp:_1992} which have been verified
by years of experiment and experience. Put another way, for certain MCNP
problems, there is a \textit{verifiably correct answer}. The community can then
build up from those basic principles to understand how ``believable'' their
solutions are for larger, more complex problems. Fuel cycle simulation does not
necessarily have these characteristics, due to the complex nature of the system
being modeled and the variety of simulation-level design decisions that
exist. Accordingly, a community consensus approach is used, which leads to
benchmarking exercises that incorporate some subset of the simulators in the
community. In general, these exercises compare large, aggregate metrics in order
to limit discrepancies caused by fundamental differences in modeling
approaches. For example, benchmarks may compare the total aggregate flow of used
fuel coming out of reactors. However, one can't really come to a consensus
answer regarding the isotopics of this spent fuel due to the wide variety of
simulator treatment of depletion and decay (ranging from no use of such methods
to full integration of depletion codes).

In his conclusions of the MIT benchmarking exercise, Guerin states that
``operation of a fuel cycle model is as much art as science''
\cite{guerin_benchmark_2009}. This statement shows the fledging state of the
overall community even after years of practice. Understanding just how much of
fuel cycle simulation is art and how much is science is critical to providing
reliable results.
